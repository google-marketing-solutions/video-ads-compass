{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yaum0ES_c_QF"
      },
      "source": [
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "# Video Ads Compass\n",
        "\n",
        "Video Ads Compass is a self-service solution that aims to help advertisers predict whether their video ads will be flagged for Google Ads Policy Violation. This tool relais on policy rules and best practices curated by the user based on their experience.\n",
        "The Video Ads Compass tool will analyze the videos against the provided rules, and will determine if and when each rule was violated in the video ad.\n",
        "Clients can use this output to avoid video ads being flagged for policy violation.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "1.  A GCP project with billing attached.\n",
        "2.  The Vertex AI API enabled.\n",
        "3.  Policy rules file in csv format, named `rules.csv` with following columns: `Rule ID, Rule Description`\n",
        "\n",
        "## Usage\n",
        "\n",
        "1.  Make a copy of this Colab notebook.\n",
        "2.  Upload the rules file to the \"Files\" tab on the left side.\n",
        "2.  Have your videos in either a GCP Cloud Storage Bucket or a Google Drive folder.\n",
        "3.  It is recommended to use a shortened version of the videos, containing only the first minute.\n",
        "4.  Run the Colab cells one by one and enter configuration details where needed.\n",
        "5.  If using a Google Drive folder as the source of videos, you need to provide the base path of this folder, depending on whether it's your drive or a shared drive, using one of these formats:\n",
        "\n",
        "    * `MyDrive/Path/To/Folder`\n",
        "    * `Shareddrive/Path/To/Folder`\n",
        "\n",
        "6.  When finished, you will receive a Google Spreadsheet URL to access the tool's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZjPnXVIc6GJ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TrX9CfESmxa"
      },
      "outputs": [],
      "source": [
        "#@title Installations\n",
        "\n",
        "!pip3 install --upgrade --user google-cloud-aiplatform\n",
        "!pip3 install ffmpeg-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiEjvNwZU36I"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "# Standard library imports\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from typing import Dict, TypedDict, List, Optional, Union\n",
        "\n",
        "# Third-party library imports\n",
        "from google.auth import default\n",
        "from google.colab import auth, drive\n",
        "from google.cloud import storage\n",
        "import gspread\n",
        "import pandas as pd\n",
        "import vertexai\n",
        "from vertexai.preview.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    Part,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvT8gfnrU-FU"
      },
      "outputs": [],
      "source": [
        "#@title Configs & Setup\n",
        "\n",
        "PROJECT_ID = \"\" #@param {type:\"string\"}\n",
        "LOCATION = \"us-central1\" #@param {type:\"string\"}\n",
        "MODEL = \"gemini-2.0-flash-001\" #@param {type:\"string\"}\n",
        "VIDEO_SOURCE = \"DRIVE\" #@param {type:\"string\"} [\"DRIVE\", \"GCS\"]\n",
        "BUCKET_NAME = \"\" #@param {type:\"string\", placeholder:\"Enter if using GCS as source\"}\n",
        "DRIVE_BASE_PATH = \"\" #@param {type:\"string\", placeholder:\"Enter if using Google Drive as source\"}\n",
        "\n",
        "_DRIVE_PATH_PREFIX = \"/content/drive/\"\n",
        "DRIVE_PATH = _DRIVE_PATH_PREFIX + DRIVE_BASE_PATH\n",
        "\n",
        "auth.authenticate_user()\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "generative_model = vertexai.generative_models.GenerativeModel(MODEL) # use the fully-qualified name\n",
        "\n",
        "def list_video_files_gcs(bucket_name: str) -> List[str]:\n",
        "    \"\"\"Lists MP4 files within the specified GCS bucket.\"\"\"\n",
        "    video_files = []\n",
        "    for blob in client.list_blobs(BUCKET_NAME):\n",
        "      if 'mp4' in blob.name:\n",
        "        video_files.append(f'gs://{BUCKET_NAME}/{blob.name}')\n",
        "    return video_files\n",
        "\n",
        "def list_video_files_drive(base_path: str) -> List[str]:\n",
        "    \"\"\"Lists MP4 files within the specified base path in Google Drive.\"\"\"\n",
        "    video_files = []\n",
        "\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"WARNING: Path does not exist: {base_path}\")\n",
        "        return []\n",
        "\n",
        "    for root, _, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.mp4'):\n",
        "                video_files.append(os.path.join(root, file))\n",
        "    return video_files\n",
        "\n",
        "\n",
        "if VIDEO_SOURCE == \"GCS\":\n",
        "  from google.cloud import storage\n",
        "  client = storage.Client()\n",
        "  all_uris = list_video_files_gcs(BUCKET_NAME)\n",
        "\n",
        "elif VIDEO_SOURCE == \"DRIVE\":\n",
        "  from google.colab import drive\n",
        "  try:\n",
        "      drive.mount('/content/drive')\n",
        "      print(\"Google Drive mounted successfully.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error mounting Google Drive: {e}\")\n",
        "      print(\"Make sure you've authorized Google Drive access.\")\n",
        "      sys.exit(1)\n",
        "  all_uris = list_video_files_drive(DRIVE_PATH)\n",
        "\n",
        "all_keys = [str(x) for x in range(len(all_uris))]\n",
        "all_uri_map = dict(zip(all_keys, all_uris))\n",
        "\n",
        "\n",
        "print('All URIs map:')\n",
        "print(all_uri_map)\n",
        "\n",
        "print(\"Script finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSyITARjTbmJ"
      },
      "source": [
        "#Shorten Videos\n",
        "### ðŸ›‘ Only run this section if you need to create a shorten version of your videos\n",
        "\n",
        "This section will create a shorten (default 1 minute) versions of all the videos in the video source you provided (DRVIE / GCS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Azwv4MTb8i"
      },
      "outputs": [],
      "source": [
        "#@title Shorten videos code\n",
        "import ffmpeg\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "new_length_seconds = 60 #@param {type: \"number\"}\n",
        "_SHORT_PREFIX = \"SHORT_\"\n",
        "\n",
        "def create_short_videos(video_dict):\n",
        "    shorten_uri_map = {}\n",
        "    # Initialize the progress bar\n",
        "    for key, uri in tqdm.tqdm(video_dict.items(), desc=\"Processing Videos\"):\n",
        "\n",
        "        base_uri, file_name = uri.split(\"/\")[-2:]\n",
        "        file_name = file_name.replace(\"SHORT_\", \"\")\n",
        "        short_file_name = _SHORT_PREFIX + file_name\n",
        "        short_uri = base_uri + '/' + short_file_name\n",
        "        if short_uri not in shorten_uri_map.values():\n",
        "            shorten_uri_map[key] = short_uri\n",
        "\n",
        "\n",
        "        if VIDEO_SOURCE == \"DRIVE\":\n",
        "            video_path = DRIVE_PATH + '/' + file_name\n",
        "            short_video_path = DRIVE_PATH + '/' + short_file_name\n",
        "            if os.path.exists(short_video_path):\n",
        "                tqdm.tqdm.write(f\"Shortened video for '{file_name}' already exists. Skipping...\")\n",
        "                continue\n",
        "\n",
        "            ffmpeg.input(video_path, t=new_length_seconds).output(short_video_path).run()\n",
        "            shorten_uri_map[key] = base_uri + '/' + short_file_name\n",
        "\n",
        "\n",
        "        elif VIDEO_SOURCE == \"GCS\":\n",
        "            # Initialize Google Cloud Storage client\n",
        "            storage_client = storage.Client()\n",
        "\n",
        "            # Check if the short video already exists in the bucket\n",
        "            bucket = storage_client.bucket(BUCKET_NAME)\n",
        "            short_blob = bucket.blob(short_file_name)\n",
        "\n",
        "            if short_blob.exists():\n",
        "                tqdm.tqdm.write(f\"Shortened video for '{file_name}' already exists. Skipping...\")\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "            # Download the video file from GCS\n",
        "            blob = bucket.blob(file_name)\n",
        "            blob.download_to_filename(file_name)\n",
        "\n",
        "            # Create the short video (only the first 1 minute)\n",
        "            ffmpeg.input(file_name, t=new_length_seconds).output(short_file_name).run()\n",
        "\n",
        "            # Upload the short video back to GCS\n",
        "            short_blob.upload_from_filename(short_file_name)\n",
        "\n",
        "            # Clean up local files\n",
        "            os.remove(file_name)\n",
        "            os.remove(short_file_name)\n",
        "\n",
        "\n",
        "    return shorten_uri_map\n",
        "\n",
        "all_uri_map = create_short_videos(all_uri_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvB6QiacdFgr"
      },
      "source": [
        "# Analyze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y71lEfEkdGmS"
      },
      "outputs": [],
      "source": [
        "#@title Load prompt and schema\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are an experienced advertising content reviewer. You specialize in determining whether or not a video ad violates google ads policy or not.\n",
        "Please conduct a strict content review of the video ad provided according to the detailed review tags - added below in csv format.\n",
        "Your judgment will directly affect the launch and promotion of this video ad, so please take every detail seriously.\n",
        "Based on the attached policy rules, analyze the video and for each policy rule determine if it violates it or not. If it does violate it, score the seriousness of the violation on a scale of 1 to 5, explain the reason for your decision, and provide the timestamp for the violation part.\n",
        "\n",
        "Video Ad Policy Review Instructions:\n",
        "- Carefully watch the entire video ad from start to finish\n",
        "- Analyze each frame and content element against the attached policy rules\n",
        "- Be thorough and objective in your assessment\n",
        "- Consider context, intent, and potential viewer interpretation\n",
        "\n",
        "Detailed Review Methodology:\n",
        "1. For each policy rule, provide a comprehensive analysis\n",
        "2. If a rule is violated, clearly document:\n",
        "   - Specific timestamp(s) of violation\n",
        "   - Exact content causing the violation\n",
        "   - Visual or contextual details\n",
        "3. Assess violation severity and potential impact\n",
        "\n",
        "Output Format (for each rule):\n",
        "{\n",
        "    \"rule_index\": int,                 # Policy rule index number\n",
        "    \"rule_violation\": bool,             # Whether rule is violated\n",
        "    \"violation_score\": int,             # Severity (1-5)\n",
        "    \"confidence_score\": float,          # Confidence in assessment (0.0-1.0)\n",
        "    \"violation_reason\": str,            # Detailed explanation\n",
        "    \"violation_timestamp\": str,         # Specific timestamp of violation\n",
        "}\n",
        "\n",
        "Final Report Requirements:\n",
        "- Overall Compliance Assessment: Provide a summary percentage of policy compliance\n",
        "- The answer for each rule according to the provided format\n",
        "\n",
        "\n",
        "Here are the review tags, in a CSV format:\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Add rules from rules.csv to prompt\n",
        "try:\n",
        "  with open('rules.csv', 'r') as file:\n",
        "    rules_content = file.read()\n",
        "    prompt += rules_content\n",
        "except FileNotFoundError:\n",
        "  print(\"rules.csv not found in the Files tab.\")\n",
        "\n",
        "\n",
        "rule_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"rule_index\": {\"type\": \"integer\"},\n",
        "        \"rule_violation\": {\"type\": \"boolean\"},\n",
        "        \"violation_score\": {\"type\": \"integer\"},\n",
        "        \"violation_reason\": {\"type\": \"string\"},\n",
        "        \"violation_time\": {\"type\": \"string\"},\n",
        "    },\n",
        "    \"required\": [\"rule_index\", \"rule_violation\", \"violation_score\", \"violation_reason\", \"violation_time\"],\n",
        "}\n",
        "\n",
        "response_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"overall_compliance_assessment\": {\"type\": \"number\"},  # Use \"number\" for float\n",
        "        \"rules\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": rule_schema,\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"overall_compliance_assessment\", \"rules\"],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQsB_3X8dd_R"
      },
      "outputs": [],
      "source": [
        "#@title Load Video Analysis Functions\n",
        "\n",
        "disclaimer_text = \"\"\"\n",
        "This analysis is for reference only and does not constitute the final\n",
        "interpretation or enforcement of Google Ads policies.\n",
        "Whether or not an ad violates the policy is still subject to the\n",
        "latest version of Google Ads policies.\n",
        "\"\"\"\n",
        "\n",
        "def analyze_video(file_path: str) -> str:\n",
        "    \"\"\"Analyzes a video file using Gemini.\"\"\"\n",
        "\n",
        "    gen_config = GenerationConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Read the video file data as bytes\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            video_data = f.read()\n",
        "\n",
        "        # Create a Part object from the video data\n",
        "        video_part = Part.from_data(data=video_data, mime_type=\"video/mp4\")\n",
        "\n",
        "        model_response = generative_model.generate_content(\n",
        "            [prompt, video_part],\n",
        "            generation_config=gen_config,\n",
        "        )\n",
        "        return model_response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing video at {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def process_videos_and_create_df(all_uri_map: Dict[str, str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes video URIs, analyzes them, and creates a flattened DataFrame.\n",
        "\n",
        "    Args:\n",
        "        all_uri_map: Dictionary mapping keys to local file paths of videos.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the flattened results.\n",
        "    \"\"\"\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for key, file_path in all_uri_map.items():\n",
        "        try:\n",
        "            result_text = analyze_video(file_path)\n",
        "            if result_text:\n",
        "                result = json.loads(result_text)\n",
        "\n",
        "                for rule in result['rules']:\n",
        "                    rule['video_type'] = 'all'\n",
        "                    rule['video_key'] = key\n",
        "                    rule['video_uri'] = file_path\n",
        "                    rule['overall_compliance_assessment'] = result['overall_compliance_assessment']\n",
        "                all_results.extend(result['rules'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing URI {file_path}: {e}\")\n",
        "\n",
        "    if not all_results:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(all_results)\n",
        "    df = df[['video_type', 'video_key', 'video_uri', 'overall_compliance_assessment',\n",
        "             'rule_index', 'rule_violation', 'violation_score', 'violation_reason', 'violation_time']]\n",
        "    return df\n",
        "\n",
        "def upload_df_to_gsheets(df: pd.DataFrame) -> str | None:\n",
        "    \"\"\"Creates a new Google Sheet, adds a merged disclaimer, uploads a DataFrame, and formats it.\n",
        "\n",
        "    Args:\n",
        "      df: The Pandas DataFrame to upload.\n",
        "\n",
        "    Returns:\n",
        "      The URL of the created Google Sheet, or None on error.\n",
        "    \"\"\"\n",
        "    cols_definition = ['video_type', 'video_key', 'video_uri', 'overall_compliance_assessment',\n",
        "                       'rule_index', 'rule_violation', 'violation_score', 'violation_reason', 'violation_time']\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"DataFrame is empty, creating sheet with disclaimer and default headers.\")\n",
        "        num_cols = len(cols_definition)\n",
        "        header_list = [cols_definition]\n",
        "        data_list = []\n",
        "    else:\n",
        "        for col in cols_definition:\n",
        "             if col not in df.columns:\n",
        "                df[col] = None\n",
        "        df = df[cols_definition]\n",
        "\n",
        "        num_cols = len(df.columns)\n",
        "        header_list = [df.columns.values.tolist()]\n",
        "        data_list = df.astype(str).values.tolist()\n",
        "\n",
        "    if num_cols == 0:\n",
        "        print(\"Error: Cannot determine columns for Google Sheet.\")\n",
        "        return None\n",
        "\n",
        "    last_col_letter = gspread.utils.rowcol_to_a1(1, num_cols)[0]\n",
        "\n",
        "    try:\n",
        "        creds, _ = default()\n",
        "        gc = gspread.authorize(creds)\n",
        "        print(\"Google Sheets API authorized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error authorizing Google Sheets API: {e}\")\n",
        "        print(\"Please ensure Application Default Credentials are set up correctly.\")\n",
        "        return None\n",
        "\n",
        "    ss_name_prefix = \"Video Ads Compass Output\"\n",
        "    spreadsheet_name = f\"{ss_name_prefix} {datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "    try:\n",
        "        print(f\"Creating spreadsheet: '{spreadsheet_name}'...\")\n",
        "        sh = gc.create(spreadsheet_name)\n",
        "        worksheet = sh.sheet1\n",
        "        print(f\"Spreadsheet created with ID: {sh.id}\")\n",
        "\n",
        "        # 1. Write Disclaimer text to A1 (it will be merged and formatted later)\n",
        "        worksheet.update_acell('A1', disclaimer_text)\n",
        "\n",
        "        # 2. Write Headers starting from A2\n",
        "        if header_list:\n",
        "            worksheet.update(header_list, range_name='A2')\n",
        "\n",
        "        # 3. Write DataFrame values starting from A3\n",
        "        if data_list:\n",
        "            print(f\"Writing {len(data_list)} data rows to spreadsheet...\")\n",
        "            worksheet.update(data_list, range_name='A3')\n",
        "        else:\n",
        "             print(\"No data rows to write.\")\n",
        "\n",
        "        # 4. Format the sheet\n",
        "        print(\"Applying formatting to spreadsheet...\")\n",
        "        return format_gsheet(sh, df, last_col_letter)\n",
        "    except gspread.exceptions.APIError as api_e:\n",
        "        print(f\"Google Sheets API Error creating/updating spreadsheet '{spreadsheet_name}': {api_e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"General Error creating/updating spreadsheet '{spreadsheet_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def format_gsheet(sh, df: pd.DataFrame, last_col_letter: str) -> str:\n",
        "    \"\"\"Formats the Google Sheet: merged disclaimer, wrapping, colors, borders, filter, and row height.\n",
        "\n",
        "    Args:\n",
        "        sh: The gspread spreadsheet object.\n",
        "        df: The original DataFrame (used for data checks).\n",
        "        last_col_letter: The letter of the last column (e.g., 'I').\n",
        "\n",
        "    Returns:\n",
        "        The spreadsheet URL.\n",
        "    \"\"\"\n",
        "    worksheet = sh.sheet1\n",
        "    num_data_rows = len(df)\n",
        "\n",
        "    header_row_index = 2\n",
        "    first_data_row_index = 3\n",
        "\n",
        "    disclaimer_range_to_merge = f'A1:{last_col_letter}1'\n",
        "    disclaimer_format_cell = 'A1'\n",
        "    header_range = f'A{header_row_index}:{last_col_letter}{header_row_index}'\n",
        "\n",
        "    if num_data_rows > 0:\n",
        "        last_data_row_index = first_data_row_index + num_data_rows - 1\n",
        "        data_range = f'A{first_data_row_index}:{last_col_letter}{last_data_row_index}'\n",
        "        full_range_inc_header = f'A{header_row_index}:{last_col_letter}{last_data_row_index}'\n",
        "    else:\n",
        "        data_range = None\n",
        "        full_range_inc_header = header_range\n",
        "\n",
        "    formatting_requests = [] # Collect requests for batch update\n",
        "\n",
        "    # 1. Merge Disclaimer Row FIRST\n",
        "    try:\n",
        "        worksheet.merge_cells(disclaimer_range_to_merge)\n",
        "    except gspread.exceptions.APIError as e:\n",
        "         print(f\"Warning: Could not merge cells {disclaimer_range_to_merge}. Error: {e}\")\n",
        "\n",
        "    # 2. Format Merged Disclaimer Cell (A1)\n",
        "    formatting_requests.append({\n",
        "        'range': disclaimer_format_cell,\n",
        "        'format': {\n",
        "            'textFormat': {'bold': True, 'fontSize': 14},\n",
        "            'wrapStrategy': 'WRAP',\n",
        "            'verticalAlignment': 'MIDDLE',\n",
        "            'horizontalAlignment': 'CENTER',\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # 3. Apply Wrapping to Headers and Data Area (if any)\n",
        "    if num_data_rows >= 0:\n",
        "         formatting_requests.append({\n",
        "             'range': full_range_inc_header,\n",
        "             'format': {\n",
        "                 'wrapStrategy': 'WRAP',\n",
        "                 'verticalAlignment': 'TOP',\n",
        "             }\n",
        "         })\n",
        "\n",
        "    # 4. Format Header Row\n",
        "    formatting_requests.append({\n",
        "        'range': header_range,\n",
        "        'format': {\n",
        "            'textFormat': {'bold': True},\n",
        "            'backgroundColor': {'red': 0.8, 'green': 0.9, 'blue': 1.0},\n",
        "            'verticalAlignment': 'MIDDLE',\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # 5. Conditional Formatting for 'rule_violation' == \"TRUE\"\n",
        "    if data_range:\n",
        "        for index, row in df.iterrows():\n",
        "            if str(row.get('rule_violation')).upper() == \"TRUE\":\n",
        "                sheet_row = first_data_row_index + index\n",
        "                formatting_requests.append({\n",
        "                    'range': f'A{sheet_row}:{last_col_letter}{sheet_row}',\n",
        "                    'format': {'backgroundColor': {'red': 1.0, 'green': 0.8, 'blue': 0.8}}\n",
        "                })\n",
        "\n",
        "\n",
        "    # 6. Group Border Formatting when 'video_key' changes\n",
        "    if data_range:\n",
        "        current_video_key = None\n",
        "        df_sorted = df.reset_index().sort_values(by=['video_key', 'index'])\n",
        "\n",
        "        for index, row in df_sorted.iterrows():\n",
        "            original_df_index = row['index']\n",
        "            sheet_row = first_data_row_index + original_df_index\n",
        "\n",
        "            video_key = row.get('video_key')\n",
        "            if video_key != current_video_key and original_df_index != 0:\n",
        "                 formatting_requests.append({\n",
        "                    'range': f'A{sheet_row}:{last_col_letter}{sheet_row}',\n",
        "                    'format': {'borders': {'top': {'style': 'SOLID_THICK', 'color': {'red': 0, 'green': 0, 'blue': 0}}}}\n",
        "                })\n",
        "            current_video_key = video_key\n",
        "\n",
        "\n",
        "    if formatting_requests:\n",
        "        print(f\"Applying {len(formatting_requests)} formatting requests via batch...\")\n",
        "        try:\n",
        "            worksheet.batch_format(formatting_requests)\n",
        "        except gspread.exceptions.APIError as e:\n",
        "            print(f\"Warning: API Error during batch formatting: {e}. Some formats might not be applied.\")\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: General Error during batch formatting: {e}.\")\n",
        "\n",
        "    # 7. Add Filters (covering headers and data)\n",
        "    try:\n",
        "        print(\"Setting basic filter...\")\n",
        "        worksheet.set_basic_filter(full_range_inc_header)\n",
        "    except gspread.exceptions.APIError as e:\n",
        "         print(f\"Warning: Could not set basic filter on {full_range_inc_header}. Error: {e}\")\n",
        "\n",
        "    return sh.url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E7knYaagrrm"
      },
      "outputs": [],
      "source": [
        "#@title Run Analysis\n",
        "\n",
        "df_results = process_videos_and_create_df(all_uri_map)\n",
        "output_url = upload_df_to_gsheets(df_results)\n",
        "print(f\"Finished Video Ads Compass analysis. Find results here: {output_url}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
